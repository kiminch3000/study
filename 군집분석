군집 분석 (Clustering)
	- 개체들의 특성에 따라 몇 개 의 군집으로 집단화하고, 형성된 군집들의 특성을 파악하여 군집들 사이의 관계를 분석하는 다변량분석
	- 거리를 측정하여 집단의 이질성, 동질성을 평가하여 그룹화
	- 분석 목적
		○ 집단 간 차이 분석
		○ 특이집단 포착(이상값 탐지)
		○ 군집의 결과를 분류분석의 입력 변수로 투입
		○ 유사한 값을 가진 많은 속성을 그룹화해 대량 데이터셋을 몇 개의 균일한 범주로 단순화
	- 관측 대상 간 거리
		○ 민코우스키 거리
			§ d(x,y)=(〖〖∑▒〖(x_i−y_i 〗)〗^m)〗^(1/m)
		○ 맨하탄 거리 (m=1)
			§ d(x,y)=∑▒|x_i−y_i | 
		○ 유클리드 거리 (m=2)
			§ d(x,y)=(〖〖∑▒〖(x_i−y_i 〗)〗^2)〗^(1/2)
		○ 체베셰프 거리(m=∞)
			§ d(x,y)=max|x_i−y_i |
		○ 표준화 거리
			§ d(x,y)=(〖〖∑▒〖(x_i−y_i 〗)〗^2/σ_i^2)〗^(1/2)
		○ 마할라노비스 거리
			§ d(x,y)=((〖〖x−y)〗^T S〗^(−1) (x−y))^(1/2),  S는 공분산행렬
		○ 캔버라 거리
			§ d(x,y)=∑▒|x_i−y_i |/(x_i+y_i )
		○ 명목형 변수일때 거리
			§ d(x,y)=(개체 x와 y에서 다른 값을 가지는 변수의 수)/(총변수의 수)
			§ 단순일치계수
			§ 자카드 계수
			§ 순위 상관 계수
			
	- 계층적 군집분석
		○ 군집화 순서에 따른 분류
			§ 병합 방법 - 가까운 관측치끼리 묶어 가는 방법 (주로 쓰임)
			§ 분할 방법 - 큰 군집으로부터 출발하여 군집을 분리해 나가는 방법
		○ 군집 간 거리 측정 방법에 따른 분류
			§ 최단연결법 - 두 집단 간 관측치들의 거리의 최솟값을 군집 간의 거리로 정의
			§ 최장연결법 - 두 집단 간 관측치들의 거리의 최댓값을 군집 간의 거리로 정의
			§ 평균연결법 - 두 집단 간 관측치들의 거리의 평균을 군집 간의 거리로 정의
			§ 중앙값연결법 - 두 집단 간 관측치들의 거리의 중앙값을 군집 간의 거리로 정의
			§ 중심연결법
			§ 와드연결법
		○ dendrogram - 군집결과를 나무구조로 나타내 군집 간의 구조관계를 알 수 있도록 한 그래프
			
	- 비계층적 군집분석
		○ 정해진 개수로 군집을 나누는데, 정해진 기준에 따라 더 이상 개선이 되지 않을 때까지 군집화 진행
		○ 군집화 과정에서 관측치가 다른 그룹으로 이동되기도 함
		○ 종류
			§ k-means clustering (Centroid-based clustering)
				□ 장단점
					® 장점
						◊ 통계 용어 없이 설명할 수 있게 군집 식별에 대한 간단한 원리를 사용
						◊ 매우 유연하며 간단한 수정으로 결점을 극복하게 적용할 수 있음
						◊ 효율적이고 데이터를 유용한 군집으로 나눔
					® 단점
						◊ 최근 군집화 알고리즘보다 덜 세련됨
						◊ 무작위 초기화 때문에 최적의 군집을 찾지 못할 수 있음
						◊ 데이터에서 얼마나 군집이 생성될 수 있을지 합리적인 추측이 필요
				□ 알고리즘
					® 기본 알고리즘
						◊ n개의 관측치에 대해 k개의 군집으로 나눈 경우
						◊ 임의로 k의 중심을 정해 각 관측치를 가장 가까운 중심에 묶어 첫 번째 군집을 만듦
						◊ 각 군집에 대해 군집 내 관측치의 거리를 계산하여 새로운 중심점을 구함
						◊ 새로 정한 중심점에 대해 각 관측치를 가까운 중심점으로 다시 묶어 군집을 재정의
						◊ 재정의된 군집이 이전 군집과 일치할 때까지 2,3을 반복
					® 초기값 선정 방법
						◊ Random
							} 임의의 좌표롤 선정 한 후 각 데이터들을 좌표의 클러스터에 배정한 후, 각 클러스터의 데이터의 평균 값을 초기값으로 선정
						◊ Forgy Approch (FA)
							} 임의의 데이터를 초기값으로 선정
						◊ Macqueen Approach (MA)
							} 임의의 데이터를 초기값으로 선정한 후, 초기값과 가장 가까운 데이터 순으로 클러스터에 포함시키며, 클러스터에 데이터가 추가 될 때마다 중심값을 재계산
							} 모든 계산이 종료된 중심점을 초기 값으로 선정
						◊ Kaufman Approach (KA)
							} 랜덤하게 초기값을 선택했을 때 발생할 수 있는 문제점을 해결
							} 데이터 집합 중 가장 중심에 위치한 데이터를 초기값으로 선택
				□ 군집 개수 선택
					® Rule of thumb
						◊ k=√(n/2)  ,  n=데이터 수
					® 엘보우(elbow 기법)
						◊ 여러 가지 k 값에 대해 군집 변경 내의 동질성과 이질성을 측정하는 방법
				◊ 클러스터 평가
					® 내부 평가
						◊ 특징
							} 데이터 집합/속성을 이용하여 클러스터 내 높은 유사도, 클러스터 간 낮은 유사도로 평가
							} 평가 점수가 높다고 해서 실제 참값에 가깝다는 보장을 할 수 없음
						◊ 종류
							} Davies-Bouldin Index
								– DB=1/n Σ_(i=1)^n   max_(j≠i)⁡((σ_i+σ_j)/d(c_i,c_j ) )
								– n : 클러스터 수
								– d(c_i,c_j )  : i와 j 클러스터 중심점 간 거리
								– σ_i  : i 클러스터 내의 모든 데이터에서 중심점까지의 거리 평균값
								– 값이 낮을 수록 좋은 클러스터링 알고리즘으로 평가
							} Dunn Index
								– D=min_(1≤i<j≤n)⁡〖d(i,j)〗/max_(1≤k≤n)⁡〖d^′ (k)〗 
								– n : 클러스터 수
								– d(c_i,c_j )  : i와 j 클러스터 중심점 간 거리
								– d^′ (k) : k 클러스터의 데이터끼리 거리가 가장 긴 값
								– 값이 높을수록 클러스터링 성능이 좋음
							} 실루엣 기법
								– s(i)=(b(i)−a(i))/max{a(i), b(i)} 
								– −1≤s(i)≤1
								– a(i) : 클러스터 내 데이터들 간의 거리 평균
								– b(i) : 속한 클러스터의 데이터를 제외한 모든 데이터들 간의 거리 평균
								– s(i)가 1에 가까울 수록 i는 올바른 클러스터
					® 외부 평가
						◊ 특징
							} 클러스터링에 사용되지 않은 데이터로 평가
							} 클러스터링 결과물이 미리 정해진 결과물과 얼마나 비슷한지 측정
						◊ 종류
							} Rand measure
								– RI=(TP+TN)/(TP+FP+FN+TN)
							} F-measure
								– F_β=((β^2+1)×P×R)/(β^2×P+R)
								– P=TP/(TP+FP)
								– R=TP/(TP+FN)
								– β≥를 변경하면서  재현율을 조정하여 FN 값의 비중을 변화
								– β=0일때 F_0=P => 재현율은 β=0 일 때 어떠한 영향을 미치지 못함
								
			§ Expectation-Maximization (EM) Clustering (Distribution-based clustering)
				
			§ Density-based spatial clustering of a plication with noise (DBSCAN)
				□ 특징
					® 장점
						◊ K-means와 같이 클러스터 수를 정하지 않아도됨
						◊ cluster에 속하는 집단이 원, 직사각형의 특정 모양을 가지지 않고, 다양한 모양을 할 수 있음
					® 단점
						◊ 고차원 데이터일수록 연산 속도가 급격히 증가
							} 유클리드 제곱거리를 사용하는 모든 데이터 모델의 공통적인 단점
						◊ 파라미터에 매우 민감
							} Eps가 너무 작으면 고밀도만 클러스터로 감지
							} Eps가 너무 크면 저밀도도 클러스터로 감지
				□ 정의
					® 이웃 포인트(neighborhood of a point)
						◊ 한 데이터로부터 반경 Eps(ε)내에 존재하는 다른 데이터
					® 코어 포인트(core point)
						◊ 이웃 포인트가 n개 이상을 가지는 데이터
						
					® 직접 접근 가능한 포인트(directly density-reachable)
						◊ 코어 포인트 p의 이웃 포인트 q에 대하여 코어 포인트 p는 이웃 포인트 q에 직접 접근 가능함이라 정의
						◊ p→q
						
					® 접근 가능한 포인트(density-reachable)
						◊ 데이터 p로부터 연속되는 이웃데이터의 연결을 배열 {p=p1, p2,…,q} 이라하며, 이를 q는 p로부터 접근 가능하다고 정의
						
					® 연결된 포인트(density-connected)
						◊ 데이터 p, q 둘다 데이터 o까지 접근 가능하면, o는 p, q와 연결된 포인트라 정의
						
					® 클러스터(Cluster)
						◊ 하나의 코어 포인트 p에 대하여 접근 가능한 데이터들의 집합
						◊ 하나의 클러스터 내의 모든 데이터들은 서로 연결된 포인트임
					® 노이즈(Noise)
						◊ 어떠한 클러스터에도 속해지지 않는 데이터 포인트
				□ 파라미터 정의
					® Eps(ε)
						◊ 임의의 점 의 밀도를 측정하기 위해 반경을 지정하는 파라미터
						◊ 보통 MinPts를 고정하고 Eps를 최적화하는 방법을 사용
					® MinPts(minimum number of points)
						◊ 임의의 점의 밀도를 측정하기 위해 Eps 반경내에 존재해야 하는 점의 최소 개수를 지정하는 파라미터
					® 밀도 구분
						◊ High density : 반경 Eps 내에 이웃한 점의 개수가 MinPts 이상인 상태
						◊ Low density : 반경 Eps 내에 이웃한 점의 개수가 MinPts 미만인 상태
					® 파라미터 Eps 최적화 개념
						◊ k
							} 자신을 제외한 k번재로 가까운 이웃점의 거리를 구하기 위한 상수
						◊ kth-neighborhood 
							} 임의의 점을 기준으로 k번재로 가까운 점
						◊ k-dist 
							} 각 점의 kth-neighborhood까지의 거리
						◊ sorted k-dist graph
							} 모든 점의 k-dist를 크기의 역순으로 정렬한 후 그 순서대로 타점을 한 그래프
			§ Fuzzy clustering 
				□ 특정 cluster에 속할 확률 계산
				□ k-means가 명확하게 cluster를 나누는 반면, fuzzy cluster는 각 cluster에 속할 확률이 계산
				□ Numeric  값만 가능
				□ NA 허용
				□ 관측치 개수/2개 까지 군집화 가능
			§ 혼합 분포 군집(Mixture distribution clustering)
				□ 모형기반 군집방법으로 데이터가 k개의 모수적 모형의 가중합으로 표현되는 모집단 모형으로부터 나왔다는 가정하에서 모수와 함께 가중치를 자료로부터 추정하는 방법을 사용
				□ 모수와 가중치의 추정에는 EM 알고리즘이 사용 (미분을 통한 이론적 전개가 어려워, 최대가능도 추정이 쉽지 않기 때문)
				□ M개 분포의 가중합으로 표현되는 혼합모형
p(x│θ)=∑24_(i=1)^M▒〖p(x│C_i,θ_i )  p(C_i)〗,  
p(x│C_i,θ_i ):혼합 모델을 이루는 단일 확률밀도 함수,  θ_i:i번째 분포의 모수 벡터,   C_i:i번째 군집,   p(C_i ):i번째 군집이 혼합모형에서 차지하는 중요도 또는 가중치
					
			§ SOM(Self-Organizing Maps, 자기조직화 지도)
				□ 자기조직화
					® 주어진 입력패턴에 대하여 정확한 해답을 미리 주지 않고 자기 스스로 학습할 수 있는 능력
				□ 비지도 신경망으로 고차원의 데이터를 이해하기 쉬운 저차원의 뉴런으로 정렬하여 지도 형태로 형상화
				□ 이러한 형상화는 입력 변수의 위치 관계를 그대로 보존한다는 특징을 가짐
				□ 인공신경망과 달리 단 하나의 전방 vom를 사용함으로써 수행 속도가 매우 빠름
				□ 구성
					® 입력층
						◊ 입력벡터를 받는 층으로 입력 변수의 개수와 동일하게 뉴런수가 존재
					® 경쟁층
						◊ 입력 벡터의 특성에 따라 벡터가 한 점으로 클러스터링 되는 층
						◊ 사용자가 미리 정해놓은 군집의 수만큼 뉴런 수가 존재
				□ 알고리즘
					® SOM 맵의 노드에 대한 연결강도를 초기화
					® 입력 벡터를 제시
					® 유클리드 거리를 사용하여 입력 벡터와 프로토타입 벡터 사이의 유사도를 계산
						◊ x−me ???
					® BMU(Best-Matching Unit)와 그 이웃들의 연결강도를 재조정
						◊ mi(t+1)=mi(t)+a(t)∙hei(t)[x−mi(t)], t : 시간, a(t)는 학습률, 
hei(t):BMU 중심에 잇는 이웃 커널exp⁡(−|(|r_e−r_i |)|^2/(eσ^2 (t) ))
						rb, ri :SOM의 그리드에서 뉴론 e와 i의 위치
					® 단계 2로 가서 반복
					
